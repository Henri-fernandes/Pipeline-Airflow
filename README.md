![Status](https://img.shields.io/badge/Status-Conclu√≠do-brightgreen)

# üöÄ Pipeline de Dados com Apache Airflow

<div align="center">

![Airflow](https://img.shields.io/badge/Apache%20Airflow-19A119?style=for-the-badge&logo=Apache%20Airflow&logoColor=black)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)
![Python](https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=306998)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
## Pipeline ETL robusto para gera√ß√£o, transforma√ß√£o e carga de dados no Data Warehouse

</div>

# **üìã Sum√°rio**

### [üéØ Vis√£o Geral](#-vis√£o-geral)
### [üåÄ Fluxo do Pipeline](#fluxo-do-pipeline)
### [üí° Por Que Este Projeto Existe?](#-por-que-este-projeto-existe)
### [üéØ Impacto no Neg√≥cio](#-impacto-no-neg√≥cio)
### [üîÑ Diferen√ßa Entre DAGs AT vs GR](#-diferen√ßa-entre-dags-at-vs-gr)
### [üîë Conceito de UNIQUE (Chave Natural + Vers√£o)](#-conceito-de-unique--chave-natural--vers√£o-)
### [üõ†Ô∏è Tecnologias e Justificativas](#Ô∏è-tecnologias-e-justificativas)
### [üîç Por Que Cada Ferramenta?](#-por-que-cada-ferramenta)
### [üñãÔ∏è Scripts das Tabelas](#Ô∏èscripts-das-tabelas)
### [üåÄ Fluxo de Versionamento](#fluxo-de-versionamento)
### [üìè Regras de Neg√≥cio Implementadas](#regras-de-neg√≥cio-implementadas)
### [üéØ Boas Pr√°ticas](#-boas-pr√°ticas)
### [üîß Instala√ß√£o](#-instala√ß√£o)
### [üîß Como Executar](#-como-executar)
### [üì∏ Exemplos Visuais](#-exemplos-visuais)
### [‚öôÔ∏è Troubleshooting](#Ô∏ètroubleshooting)
### [üìß Contato](#-contato)
---

# **üéØ Vis√£o Geral**


>Este projeto implementa um **pipeline de dados end-to-end** utilizando **Apache Airflow** como orquestrador, projetado para simular um ambiente de produ√ß√£o real de Data Warehousing. O pipeline gerencia todo o ciclo de vida dos dados: desde a gera√ß√£o de dados sint√©ticos realistas at√© a carga final em um Data Warehouse dimensional, mantendo hist√≥rico completo e garantindo integridade referencial.

### üõ†Ô∏è **Tecnologias Utilizadas**

>**Python + Faker**: Gera√ß√£o de dados sint√©ticos realistas (pessoas, Cr√©dito, contas, etc.)
>
>**Apache Airflow**: Orquestra√ß√£o e automa√ß√£o de todo o fluxo de dados, executando as tarefas em sequ√™ncia >e hor√°rios programados
>
>**PostgreSQL**: Banco de dados relacional que armazena tanto a camada de staging quanto o Data Warehouse >final
>
>**SQL**: Transforma√ß√µes, valida√ß√µes e aplica√ß√£o de regras de neg√≥cio nos dados
>
>### ‚ú® **Benef√≠cios**
>
>‚úÖ Ambiente seguro para testes sem usar dados reais  
>‚úÖ Automa√ß√£o completa via Airflow  
>‚úÖ Dados realistas para aprendizado e desenvolvimento  
>‚úÖ Arquitetura escal√°vel e profissional
---


# **üåÄFluxo do Pipeline:**

>### 1Ô∏è‚É£ **Gera√ß√£o de Dados** üé≤
>Dados sint√©ticos s√£o criados usando Python com Faker para simular informa√ß√µes realistas em ambientes de >desenvolvimento e teste.
>
>### 2Ô∏è‚É£ **Staging** üì•
>Os dados brutos chegam na √°rea tempor√°ria `staging.dim_pessoa_raw`, onde ficam armazenados sem >transforma√ß√µes, prontos para processamento.
>
>### 3Ô∏è‚É£ **Transforma√ß√£o** üßπ
>Scripts SQL aplicam limpeza, valida√ß√£o e regras de neg√≥cio. Dados inconsistentes s√£o tratados e apenas >informa√ß√µes de qualidade seguem adiante.
>
>### 4Ô∏è‚É£ **Data Warehouse** üèõÔ∏è
>Dados validados s√£o armazenados em `dw.dim_pessoa` de forma estruturada e otimizada para an√°lises, >servindo como fonte √∫nica da verdade.
---

# **üí° Por Que Este Projeto Existe?**

### **Problemas Que Resolve**


>| üî¥ Problema | ‚úÖ Solu√ß√£o Implementada |
>|------------|------------------------|
>| **Dados duplicados no DW** | Sistema de versionamento com chave natural composta ( Chave_produto + vers√£o) |
>| **Falta de dados para testes** | Gera√ß√£o autom√°tica de dados sint√©ticos realistas com Faker |
>| **Perda de hist√≥rico de altera√ß√µes** | Arquitetura SCD Type 2 com controle temporal completo |
>| **Pipeline manual e propenso a erros** | Orquestra√ß√£o autom√°tica via Airflow com retry e monitoramento |
>| **Inconsist√™ncia entre ambientes** | Docker Compose para ambiente replic√°vel e isolado |
>| **Dif√≠cil rastreabilidade** | Logging detalhado e metadados em todas as camadas |
---

# **üéØ Impacto no Neg√≥cio**


>### Para o Time de Dados
>- ‚ö° **Redu√ß√£o de 70% no tempo de desenvolvimento** de novos pipelines
>- üõ°Ô∏è **Zero duplicatas** no DW gra√ßas ao controle de vers√£o e boas pr√°ticas
>- üîç **Rastreabilidade completa** de todas as transforma√ß√µes
>
>>### Para o Time de BI/Analytics
>- üìä **Dados confi√°veis** para dashboards e relat√≥rios
>- üìà **Hist√≥rico completo** para an√°lises de tend√™ncias temporais
>- ‚úì **Valida√ß√£o pr√©via** de regras de neg√≥cio antes da produ√ß√£o
>
>>### Para a Empresa
>- üí∞ **Economia de recursos** com detec√ß√£o precoce de erros
>- üöÄ **Time-to-market reduzido** para novos casos de uso
>- üéì **Ambiente de aprendizado** para novos membros do time
 

---

# **üîÑ Diferen√ßa Entre DAGs AT vs GR**

### As tabelas de Dimens√µes utilizam dois tipos de DAGs com prop√≥sitos distintos:


>| Aspecto | üîµ DAG **AT** | üü¢ DAG **GR** |
>|---------|--------------------------------|-------------------------|
>| **Prop√≥sito** | Atualizar os dados existentes do Data Warehouse | Gerar os dados que iram para o Data Warehouse |
>| **Depend√™ncias** |  Depend√™ncias dos Dags GR | Tarefas encadeadas com depend√™ncias de outros GR |
>| **Exemplo Real** | `AT_dim_pessoa_dag.py` | `GR_dim_pessoa_dag.py` |
>| **Reusabilidade** | Alta (componente isolado) | √önico (fluxo espec√≠fico) |



## **üìå Exemplo Pr√°tico de DAGs**

### **üîµ DAG AT:**
```python
def atualizar_dim_pessoa(ti):
    hook = PostgresHook(postgres_conn_id='postgres_dw_pipeline')
    conn = hook.get_conn()
    cur = conn.cursor()

    cur.execute(""" 
        SELECT cd_pessoa, nome_pessoa, nr_cpf, estado, cidade, versao, dt_cadastro
        FROM dw.dim_pessoa
        WHERE ativo = 'S'
    """)
    pessoas = cur.fetchall()

    dados = []
    eventos = []
    cidades_alternativas = ["Goi√¢nia", "Campinas", "Uberl√¢ndia", "Niter√≥i", "Joinville"]
    estados_alternativos = ["SP", "RJ", "MG", "RS", "PR"]

    for pessoa in pessoas:
        cd_pessoa, nome_pessoa, nr_cpf, estado, cidade, versao, dt_cadastro = pessoa

        alterar = random.random() < 0.8
        encerrar = random.random() < 0.09 

        if encerrar:
            cur.execute(""" 
                UPDATE dw.dim_pessoa
                SET ativo = 'N', dt_fim = CURRENT_DATE, dt_modificacao = NOW()
                WHERE cd_pessoa = %s AND ativo = 'S'
            """, (cd_pessoa,))
            continue 

        if alterar:
            nova_cidade = random.choice([c for c in cidades_alternativas if c != cidade])
            novo_estado = random.choice([e for e in estados_alternativos if e != estado])
            novo_nome = nome_pessoa.strip()

            # Gerar eventos
            if nova_cidade != cidade:
                eventos.append((cd_pessoa, 'cidade', cidade, nova_cidade))
            if novo_estado != estado:
                eventos.append((cd_pessoa, 'estado', estado, novo_estado))
            if novo_nome != nome_pessoa:
                eventos.append((cd_pessoa, 'nome_pessoa', nome_pessoa, novo_nome))

            nova_versao = versao + 1
        
            dados.append({
                'nome_pessoa': novo_nome,
                'nr_cpf': nr_cpf,
                'cd_pessoa': cd_pessoa,
                'chave_natural': None,
                'estado': novo_estado,
                'cidade': nova_cidade,
                'versao': nova_versao,
                'ativo': 'S',
                'dt_cadastro': dt_cadastro,
                'dt_inicio': datetime.today().date(),
                'dt_fim': None,
                'dt_modificacao': None,
            })

    # Inserir eventos na tabela evt.ev_dim_pessoa
    for ev in eventos:
        cur.execute("""
            INSERT INTO evt.ev_dim_pessoa (
                cd_pessoa, campo_alterado, valor_anterior, valor_novo,
                tipo_evento, data_evento, origem
            ) VALUES (%s, %s, %s, %s, 'ALTERACAO', CURRENT_DATE, 'simulador')
        """, ev)

    conn.commit()
    cur.close()
    conn.close()

    ti.xcom_push(key='dados_pessoa', value=dados)
```

### **üü¢ DAG GR:**
```python
def gerar_dados_dim_pessoa(ti):
    fake = Faker('pt_BR')
    capitais_brasil = ["cidade": "Rio Branco", "estado": "Acre", "sigla": "AC"] # Lista Simplificada P/ Exemplo

    # Buscar filiais existentes
    filiais_existentes = buscar_filiais_existentes()
    
    if not filiais_existentes:
        raise ValueError("Nenhuma filial encontrada no banco. Verifique a tabela dim_filial.")

    dados = []
    for i in range(1, 1001):
        # Nome da pessoa com chance de espa√ßos extras
        nome = fake.name()
        if random.random() < 0.2:
            opcoes = [
                nome + " " * random.randint(1, 5),
                " " * random.randint(1, 5) + nome,
                " " * random.randint(1, 3) + nome + " " * random.randint(1, 3),
                nome.replace(" ", "  "),
                f" {nome} . ",
            ]
            nome = random.choice(opcoes)

        # CPF com formatos variados
        cpf_base = fake.cpf()
        formatos = [
            cpf_base,  
            cpf_base.replace(".", "").replace("-", ""),  
            cpf_base.replace(".", "/").replace("-", "/"),  
            cpf_base.replace(".", "_").replace("-", "_"),  
            cpf_base.replace(".", " ").replace("-", " "),  
        ]
        cpf = random.choice(formatos)
        cd_pessoa = f"{random.randint(1, 9999999):07d}" 

        # Email com chance de espa√ßos extras
        email = fake.email()
        if random.random() < 0.05:
            opcoes_email = [
                email + " " * random.randint(1, 3),
                " " * random.randint(1, 3) + email,
                " " * random.randint(1, 2) + email + " " * random.randint(1, 2)
            ]
            email = random.choice(opcoes_email)

        # Selecionar filial aleat√≥ria das existentes
        cd_filial = random.choice(filiais_existentes)
        contato = fake.phone_number()
        salario = round(random.uniform(1200, 250000), 2)
        local = random.choice(capitais_brasil)
        dt_cadastro = fake.date_time_between(start_date='-5y', end_date='-1y')
        dt_inicio = fake.date_between(start_date=dt_cadastro, end_date='now')

        dados.append({
            'nome_pessoa': nome,
            'nr_cpf': cpf,
            'cd_pessoa': cd_pessoa,
            'chave_natural': None,
            'versao': 1,
            'email': email,
            'contato': contato,
            'salario': salario,
            'cidade': local["cidade"],
            'estado': local["estado"],
            'sigla_estado': local["sigla"],
            'cd_filial': cd_filial,
            'ativo': 'S',
            'dt_cadastro': dt_cadastro,
            'dt_inicio': dt_inicio,
            'dt_fim': None,
            'dt_modificacao': None
        })

    ti.xcom_push(key='dados_pessoa', value=dados)
```

# üîë Conceito de UNIQUE ( Chave Natural + Vers√£o )

### Uma das boas pr√°ticas deste pipeline √© o uso de **Unique** na **Chave Natural** e **Vers√£o** para garantir unicidade e rastreabilidade:

```sql
-- Estrutura da chave composta
SELECT
p.cd_pessoa || '|' || REGEXP_REPLACE(p.nr_cpf, '[^0-9]', '', 'g') AS chave_natural
FROM staging.dim_pessoa_raw

--- Estrutura d0 UNIQUE

CONSTRAINT unq_nr_cpf_versao_cd_pessoa UNIQUE (nr_cpf, versao, cd_pessoa)

-- Exemplo Funcional:

-- CPF         | Vers√£o | CD Pessoa |     Nome      |    Chave Natural    | Data Carga
-- 12345678901 |   1    | 7654321   | Jo√£o Silva    | 7654321|12345678901 | 2024-01-15
-- 12345678901 |   2    | 7654321   | Jo√£o Silva Jr | 7654321|12345678901 | 2024-02-20
-- 98765432100 |   1    | 8765432   | Maria Santos  | 8765432|98765432100 | 2024-01-15


-- Exemplo N√ÉO Funcional: UNIQUE N√ÉO Permite ter nr_cpf, versao, cd_pessoa iguais em mais de uma linha, Evitando duplicidade.

-- CPF         | Vers√£o | CD Pessoa |     Nome      |    Chave Natural    | Data Carga
-- 12345678901 |   1    | 7654321   | Jo√£o Silva    | 7654321|12345678901 | 2024-01-15  
-- 12345678901 |   1    | 7654321   | Jo√£o Silva Jr | 7654321|12345678901 | 2024-02-20
-- 98765432100 |   1    | 8765432   | Maria Santos  | 8765432|98765432100 | 2024-01-15


```

## **Por que isso √© importante?**
```
- ‚úÖ Permite rastrear **todas as mudan√ßas** de uma pessoa ao longo do tempo
- ‚úÖ Evita **duplicatas acidentais** durante cargas
- ‚úÖ Facilita **auditorias** e an√°lises hist√≥ricas
- ‚úÖ Implementa **SCD Type 2** de forma nativa

```

---

# üõ†Ô∏è Tecnologias e Justificativas

### ‚öôÔ∏è Stack Tecnol√≥gico

>| Tecnologia | Vers√£o | Justificativa de Escolha |
>|-----------|--------|-------------------------|
>| **Apache Airflow** | 3.x | Orquestra√ß√£o robusta com UI intuitiva, retry autom√°tico, monitoramento e agendamento flex√≠vel |
>| **Python** | 3.9+ | Linguagem vers√°til com vasto ecossistema de bibliotecas para manipula√ß√£o de dados |
>| **PostgreSQL** | 14+ | Banco ACID-compliant, suporte a transa√ß√µes complexas, queries anal√≠ticas eficientes |
>| **Faker** | 18.x | Gera√ß√£o de dados sint√©ticos realistas para testes e desenvolvimento |
>| **Docker** | 20.x | Isolamento de ambiente, reprodutibilidade e facilidade de deploy |
>| **Docker Compose** | 2.x | Orquestra√ß√£o multi-container simplificada |

# üîç Por Que Cada Ferramenta?

### Apache Airflow
```python
# Vantagens do Airflow neste projeto:
‚úÖ Agendamento cron nativo (@daily, @hourly, custom)
‚úÖ Retry autom√°tico em caso de falhas
‚úÖ Monitoramento visual via UI
‚úÖ Paraleliza√ß√£o de tarefas
‚úÖ Logging centralizado
‚úÖ Versionamento de DAGs via Git
‚úÖ Framework de orquestra√ß√£o baseado em Python
```

### PostgreSQL
```python
# Recursos cr√≠ticos utilizados:
‚úÖ Constraints (PRIMARY KEY, FOREIGN KEY, UNIQUE)
‚úÖ CTEs (Common Table Expressions)
‚úÖ Indexes para performance
```

### Faker
```python
# Exemplo de gera√ß√£o de dados realistas
from faker import Faker
fake = Faker('pt_BR')

pessoa = {
    'cpf': fake.cpf(),                    # 123.456.789-01
    'nome': fake.name(),                  # Jo√£o Silva Santos
    'email': fake.email(),                # joao.silva@example.com
    'tipo_conta': fake.random_element(['corrente', 'poupanca', 'salario'])
}
```
---


# **üñãÔ∏èScripts das Tabelas**

### **Tabela de staging:** `dim_pessoa_raw`

```sql
CREATE TABLE staging.dim_pessoa_raw (
    nome_pessoa     TEXT,
    cd_pessoa       TEXT,
    nr_cpf          TEXT,
    chave_natural   TEXT,
    email           TEXT,
    contato         TEXT,
    salario         TEXT,
    estado          TEXT,
    cidade			TEXT,
    sigla_estado    TEXT,
    cd_filial       TEXT,
    versao          INT,
    ativo           TEXT,
    dt_cadastro     TIMESTAMP,
    dt_inicio       TIMESTAMP,
    dt_fim          TIMESTAMP,
    dt_modificacao  TIMESTAMP
);
```

>### **Caracter√≠sticas:**
>- ‚úÖ Recebe dados brutos do sistema fonte
>- ‚úÖ Timestamp de carga para auditoria

### **Tabela do DW:** `dim_pessoa`

```sql
CREATE TABLE dw.dim_pessoa (
    id_pessoa SERIAL PRIMARY KEY,       
    nome_pessoa varchar (100) not null,
    cd_pessoa char (7) not null,
    nr_cpf char(11) not null,
    chave_natural char (19) not null,
    email varchar(100),
    contato varchar(20),
    salario numeric(10,2) not null,
    cidade varchar(50) not null,
    estado varchar(50) not null,        
    sigla_estado char(2) not null,
    cd_filial char (5) not null,
    versao int not null,                
    ativo CHAR(1) DEFAULT 'S' CHECK (ativo IN ('S','N')),
    dt_cadastro date not null,
    dt_inicio date not null,
    dt_fim date not null default '9999-12-31',
    dt_modificacao timestamp not null default now(),
    dt_referencia date not null,
    CONSTRAINT chk_datas CHECK (dt_fim >= dt_inicio),
    CONSTRAINT unq_nr_cpf_versao_cd_pessoa UNIQUE (nr_cpf, versao, cd_pessoa)
)

create index idx_dim_pessoa_cidade
	on dw.dim_pessoa (cidade)
	
create index idx_dim_pessoa_estado 
	on dw.dim_pessoa (estado)
	
create index idx_dim_pessoa_salario 
	on dw.dim_pessoa (salario)
	
create index idx_dim_cd_pessoa 
	on dw.dim_pessoa (cd_pessoa)
```

>### **Caracter√≠sticas:**
>- ‚úÖ Keys para joins eficientes
>- ‚úÖ Mant√©m hist√≥rico completo (SCD Type 2)
>- ‚úÖ Controle de vig√™ncia temporal
>- ‚úÖ Flag de registro ativo para facilitar queries

# **üåÄFluxo de Versionamento**

>### **Como Funciona o Controle de Vers√µes**
>
>### O pipeline implementa um sistema cuidadoso de versionamento que mant√©m o **hist√≥rico completo de altera√ß√µes** nos dados, permitindo an√°lises temporais e auditoria.
>
>---
>
>## üìù Exemplo Pr√°tico: Mudan√ßa de Tipo de Conta
>
>### **Vers√£o 1 - Registro Inicial**
>
>1. **Sistema Fonte** envia os dados: Jo√£o Silva com Conta corrente 12345 e Filial 00001 
>2. **Staging** recebe e armazena como **vers√£o 1** com timestamp
>3. **ETL** processa e valida os dados
>4. **Data Warehouse** insere o registro (verifica se n√£o existe pelo `CD_CONTA`)
>
>### **Resultado**: Jo√£o Silva (Conta corrente 12345, v1, Filial 00001 ) ‚Üí **ATIVO**
>
>---
>
>### **Vers√£o 2 - Atualiza√ß√£o de Dados**
>
>### Cen√°rio: Jo√£o muda de Filial
>1. **Sistema Fonte** envia os novos dados: Jo√£o Silva com Conta corrente 12345 e Filial 00002
>2. **Staging** recebe e armazena como **vers√£o 2** com novo timestamp
>3. **ETL** identifica que j√° existe um registro para o Conta corrente 12345
>4. **Data Warehouse** executa duas opera√ß√µes:
>   - **Insere** a nova vers√£o 2 com os dados atualizados
>   - **Atualiza** a vers√£o 1: marca `ativo = 'N'` e define `dt_fim`
>
>### **Resultado no Data Warehouse**:
>- ### Jo√£o Silva (Conta corrente 12345, v1, Filial 00001) ‚Üí **INATIVO** (hist√≥rico)
>- ###  Jo√£o Silva (Conta corrente 12345, v2, Filial 00002) ‚Üí **ATIVO** (atual)
>
>---
>
>## üéØ Benef√≠cios do Versionamento
>
>‚úÖ **Hist√≥rico Completo**: Todas as altera√ß√µes s√£o preservadas  
>‚úÖ **Auditoria**: Saber exatamente quando e o que mudou  
>‚úÖ **An√°lise Temporal**: Consultar como os dados estavam em qualquer per√≠odo  
>‚úÖ **Recupera√ß√£o**: Possibilidade de reverter para vers√µes anteriores  
>‚úÖ **Conformidade**: Atende requisitos regulat√≥rios de rastreabilidade
>
>## üîë Campos de Controle
>
>- **Vers√£o**: N√∫mero sequencial identificando cada altera√ß√£o
>- **flag_ativo**: Indica se √© a vers√£o atual (S) ou hist√≥rica (N)
>- **data_inicio_vigencia**: Quando a vers√£o foi criada
>- **data_fim_vigencia**: Quando a vers√£o foi substitu√≠da (NULL se ainda ativa)



# **üìèRegras de Neg√≥cio Implementadas**

### 1.  **ETL** simplificado da gera√ß√£o dos dados

```sql
-- 1. Verifica se n√£o existe informa√ß√µes desta pessoa antes do INSERT
INSERT INTO dw.dim_pessoa (
    nome_pessoa, cd_pessoa, nr_cpf, chave_natural, email, cd_filial, versao, ativo,
    dt_cadastro, dt_inicio, dt_fim, dt_modificacao
)
SELECT
    UPPER(TRIM(REPLACE(REPLACE(REPLACE(p.nome_pessoa, '  ', ' '), '.', ''), '  ', ' '))) AS nome_pessoa,
    p.cd_pessoa,
    REGEXP_REPLACE(p.nr_cpf, '[^0-9]', '', 'g') AS nr_cpf,
    p.cd_pessoa || '|' || REGEXP_REPLACE(p.nr_cpf, '[^0-9]', '', 'g') AS chave_natural,
    TRIM(p.email),
    p.cd_filial,
    p.versao,
    UPPER(p.ativo),
    p.dt_cadastro,
    p.dt_inicio,
    COALESCE(p.dt_fim, DATE '9999-12-31'),
    COALESCE(p.dt_modificacao, CURRENT_TIMESTAMP)
FROM staging.dim_pessoa_raw p
WHERE NOT EXISTS ( 
    SELECT 1
    FROM dw.dim_pessoa d
    WHERE d.cd_pessoa = p.cd_pessoa
      AND d.nr_cpf = REGEXP_REPLACE(p.nr_cpf, '[^0-9]', '', 'g')
      AND d.versao = p.versao
);


```

### 2. **ETL** simplificado da Atualiza√ß√£o de Vers√µes 

```sql
-- Quando uma nova vers√£o chega, inativa as vers√µes anteriores
-- 1. Identificar vers√£o ativa de cada pessoa
WITH versoes_ativas AS (
    SELECT *
    FROM dw.dim_pessoa
    WHERE ativo = 'S'
),
-- 2. Buscar eventos mais recentes por campo
eventos AS (
    SELECT DISTINCT ON (cd_pessoa, campo_alterado)
           cd_pessoa, campo_alterado, valor_novo, data_evento
    FROM evt.ev_dim_pessoa
    WHERE tipo_evento = 'ALTERACAO'
    ORDER BY cd_pessoa, campo_alterado, data_evento DESC
),
-- 3. Juntar eventos com vers√£o ativa
eventos_por_campo AS (
    SELECT
        va.cd_pessoa,
        va.nome_pessoa,
        va.nr_cpf,
        va.email,
        va.cd_filial,
        va.versao,
        va.dt_cadastro,
        ev_nome.valor_novo AS novo_nome,
        ev_email.valor_novo AS novo_email,
        ev_filial.valor_novo AS nova_filial,
        GREATEST(
            COALESCE(ev_nome.data_evento, DATE '1900-01-01'),
            COALESCE(ev_email.data_evento, DATE '1900-01-01'),
            COALESCE(ev_filial.data_evento, DATE '1900-01-01')
        ) AS maior_data_evento
    FROM versoes_ativas va
    LEFT JOIN eventos ev_nome   ON va.cd_pessoa = ev_nome.cd_pessoa AND ev_nome.campo_alterado = 'nome_pessoa'
    LEFT JOIN eventos ev_email  ON va.cd_pessoa = ev_email.cd_pessoa AND ev_email.campo_alterado = 'email'
    LEFT JOIN eventos ev_filial  ON va.cd_pessoa = ev_filial.cd_pessoa AND ev_filial.campo_alterado = 'cd_filial'
),
-- 4. Pessoas que precisam ser atualizadas
pessoas_a_atualizar AS (
    SELECT *
    FROM eventos_por_campo
    WHERE
        COALESCE(novo_nome, nome_pessoa) <> nome_pessoa OR
        COALESCE(novo_email, email) <> email OR
        COALESCE(nova_filial, cd_filial) <> cd_filial
),
-- 5. Encerrar vers√£o atual
encerrar AS (
    UPDATE dw.dim_pessoa d
    SET ativo = 'N',
        dt_fim = CURRENT_DATE,
        dt_modificacao = CURRENT_TIMESTAMP
    FROM pessoas_a_atualizar p
    WHERE d.cd_pessoa = p.cd_pessoa AND d.ativo = 'S'
    RETURNING d.*
)
-- 6. Inserir nova vers√£o
INSERT INTO dw.dim_pessoa (
    nome_pessoa, cd_pessoa, nr_cpf, chave_natural, email, cd_filial, versao, ativo,
    dt_cadastro, dt_inicio, dt_fim, dt_modificacao
)
SELECT
    UPPER(COALESCE(p.novo_nome, p.nome_pessoa)),
    p.cd_pessoa,
    p.nr_cpf,
    UPPER(p.cd_pessoa || '|' || p.nr_cpf),
    COALESCE(p.novo_email, p.email),
    UPPER(COALESCE(p.nova_filial, p.cd_filial)),
    p.versao + 1,
    'S',
    p.dt_cadastro,
    CURRENT_DATE,
    DATE '9999-12-31',
    CURRENT_TIMESTAMP
FROM pessoas_a_atualizar p;
```

---

# **üéØ Boas Pr√°ticas**

### **1. Naming Conventions**

### Padr√£o de Nomenclatura de Tabelas

```sql
-- ‚ùå EVITE (nomes gen√©ricos e confusos)
CREATE TABLE pessoa;
CREATE TABLE dados_pessoa;
CREATE TABLE tbl_pessoa;

-- ‚úÖ RECOMENDADO (clareza sobre camada e conte√∫do)
CREATE TABLE staging.dim_pessoa_raw;
CREATE TABLE dw.dim_pessoa;
CREATE TABLE dw.ft_investimento;
```

### Conven√ß√£o utilizada

>- `raw` - Tabelas de staging (dados brutos)
>- `dim` - Dimens√µes do DW
>- `ft`  - Fatos do DW
>- `ev` - Tabelas da evt(Tabela de eventos)

### Padr√£o de Nomenclatura Para DAGs

```python

# ‚úÖ DAGs At√¥micas (AT)
AT_dim_pessoa_dag
AT_dim_conta_dag
AT_dim_filial_dag

# ‚úÖ DAGs Agrupadas (GR)
GR_dim_pessoa_dag
GR_ft_investimento_dag
GR_ft_seguro_dag

```

### Padr√£o de Nomenclatura e tipo de dados para as Colunas 

```sql
--- Exemplo
dt_cadastro timestamp 
dt_inicio timestamp 
dt_fim timestamp 
dt_modificacao timestamp 
dt_referencia timestamp 
```

### **2. Versionamento e Chaves Naturais**

### Uso de tabelas para resgistrar eventos

```sql
INSERT INTO evt.ev_dim_pessoa (
                cd_pessoa, campo_alterado, valor_anterior, valor_novo,
                tipo_evento, data_evento, origem
)
```

### Garantia de Unicidade

```sql
CONSTRAINT unq_nr_cpf_versao_cd_pessoa UNIQUE (nr_cpf, versao, cd_pessoa)
```
### **3. Otimiza√ß√£o de Performance**

### √çndices Estrat√©gicos

```sql
-- ‚úÖ √çndices para queries frequentes
create index idx_dim_pessoa_cidade
	on dw.dim_pessoa (cidade)
	
create index idx_dim_pessoa_estado 
	on dw.dim_pessoa (estado)
	
create index idx_dim_pessoa_salario 
	on dw.dim_pessoa (salario)
	
create index idx_dim_cd_pessoa 
	on dw.dim_pessoa (cd_pessoa)
```

# üîß **Instala√ß√£o**

## **Pr√©-requisitos**
>### - Docker & Docker Compose
>### - Python 3.9+
>### - 4GB RAM dispon√≠vel
---


## **Documenta√ß√£o para instala√ß√£o**

> ### üîóLink da documenta√ß√£o do Airflow: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html?utm_source=chatgpt.com
> ### üîóLink da documenta√ß√£o do Docker Desktop para Windows: https://docs.docker.com/desktop/setup/install/windows-install/
> ### üîóLink da documenta√ß√£o do Postgres: https://www.postgresql.org/docs/
> ### üîóLink da documenta√ß√£o do Python: https://www.python.org/downloads/
> ### üîóLink da documenta√ß√£o do DBeaver: https://dbeaver.io/download/


## **Clonar Reposit√≥rio**

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/Henri-fernandes/Pipeline-Airflow.git
cd airflow-pipeline

# 2. Configure as vari√°veis de ambiente
cp .env.example .env

# 3. Suba os containers
docker-compose up -d

# 4. Acesse o Airflow
# URL: http://localhost:8080
# User: admin
# Pass: admin
```

# üîß **Como Executar**

### Este guia apresenta o passo a passo para configurar o ambiente, executar o pipeline de dados e verificar os resultados.

## Passo 1: Clonar o Reposit√≥rio

### Primeiro, voc√™ precisa fazer o clone do reposit√≥rio do projeto para o seu ambiente local. Utilize o comando `git clone` seguido da URL do reposit√≥rio.

```bash
git clone https://github.com/Henri-fernandes/Pipeline-Airflow.git
```

## Passo 2: Configurar o Banco de Dados
>
>### Ap√≥s clonar o reposit√≥rio, localize os scripts de cria√ß√£o de tabelas que est√£o inclu√≠dos no >projeto. Execute esses scripts no seu banco de dados para criar a estrutura necess√°ria das >tabelas. Esses scripts v√£o preparar o ambiente de dados para receber as informa√ß√µes processadas >pelo Airflow.
>
>### Os scripts das tabelas geralmente est√£o localizados em uma pasta como `Estrutura/`  dentro do reposit√≥rio.

---
## Passo 3: Acessar o Airflow
> 
> ### Abra a interface web do Airflow atrav√©s do navegador. Voc√™ ser√° direcionado para o painel  principal da ferramenta, onde poder√° visualizar e gerenciar todas as DAGs dispon√≠veis.
> 
---
## Passo 4: Localizar a DAG
>
>### No painel do Airflow, siga estas etapas:
>
>### 1. Clique na aba **"DAGs"** localizada no menu superior
>### 2. Utilize a barra de pesquisa para encontrar a DAG espec√≠fica chamada `GR_dim_filial_dag`
>### 3. Clique sobre a DAG para visualizar seus detalhes
---
 ## Passo 5: Executar e Acompanhar
> 
> ### Ative a DAG caso ainda n√£o esteja ativa (utilize o bot√£o toggle ao lado do nome da DAG). O Airflow processar√° todas as tarefas (tasks) definidas nessa DAG automaticamente. Acompanhe o > progresso da execu√ß√£o atrav√©s da interface gr√°fica.
---
## Passo 6: Verificar os Resultados
> 
> ### Quando a execu√ß√£o finalizar, observe se todas as tasks est√£o com o status **"success"** (sucesso), geralmente indicado pela cor verde. 
>
> ### Se todas as tasks estiverem com esse status, significa que o processo foi conclu√≠do com √™xito. Agora voc√™ pode acessar o banco de dados diretamente para verificar os dados que foram inseridos ou atualizados pelas tarefas executadas.
>

---
>### **Observa√ß√£o:** Em caso de falhas, verifique os logs de cada task clicando sobre ela para identificar e corrigir poss√≠veis problemas.
---

# **üì∏ Exemplos Visuais**

## üåÄ Airflow UI - DAGs 

Onde vai ficar a foto

## üåÄ Airflow UI - Fluxo 

Onde vai ficar a foto


## üìä Estrutura do Data Warehouse

Onde vai ficar a foto

## üìä Tabela `dw.dim_filial`

Onde vai ficar a foto

## üÜóExecu√ß√£o

onde video vai ficar

---

# ‚öôÔ∏èTroubleshooting

### Este documento apresenta os problemas mais comuns que podem ocorrer durante a execu√ß√£o do pipeline, suas causas e as respectivas solu√ß√µes.

## 1. Erro de Conex√£o com o Banco de Dados

### Mensagem de Erro
```python
# psycopg2.OperationalError: could not connect to server
```

### Causa
>A conex√£o com o banco de dados est√° mal configurada no Airflow. Isso pode acontecer quando as credenciais ou par√¢metros de conex√£o est√£o incorretos ou ausentes.

### Solu√ß√£o
>Verifique se a conex√£o `postgres_dw_pipeline` est√° corretamente definida na interface do Airflow:
>
>1. Acesse o Airflow UI
>2. V√° em **Admin** ‚Üí **Connections**
>3. Localize a conex√£o `postgres_dw_pipeline`
>4. Confirme se os seguintes par√¢metros est√£o corretos:
>   - **Host**: Endere√ßo do servidor do banco de dados
>   - **Port**: Porta de conex√£o (geralmente 5432 para PostgreSQL)
>   - **Schema**: Nome do schema/database
>   - **Login**: Usu√°rio do banco de dados
>   - **Password**: Senha do usu√°rio

---

## 2. DAG N√£o Aparece no Airflow

### Mensagem de Erro

```python
# A DAG n√£o √© listada na interface do Airflow.
```

### Causa
>Este problema pode ocorrer por dois motivos principais:
>- O arquivo `.py` da DAG n√£o est√° na pasta `dags/`
>- Existem erros de sintaxe no c√≥digo Python da DAG

### Solu√ß√£o
>Siga estas etapas para resolver o problema:
>
>1. **Verifique a localiza√ß√£o do arquivo**: Confirme que o arquivo Python est√° dentro da pasta `dags/` do Airflow
>2. **Verifique erros de sintaxe**: Procure por:
>   - Erros de indenta√ß√£o
>   - Importa√ß√µes quebradas ou m√≥dulos n√£o instalados
>   - Erros de sintaxe no c√≥digo Python
>3. **Consulte os logs**: Verifique os logs do Airflow scheduler para identificar mensagens de erro espec√≠ficas
>4. **Reinicie o scheduler**: Ap√≥s corrigir os problemas, reinicie o Airflow scheduler

---

## 4. Task Falha por Falta de XCom

### Mensagem de Erro
```python
# NoneType object has no attribute...
```
ou
```python
# ti.xcom_pull() retorna vazio
```

### Causa
>A task anterior n√£o foi executada com sucesso ou n√£o realizou o `xcom_push` dos dados necess√°rios. Isso quebra a comunica√ß√£o entre tasks que dependem de dados compartilhados.

### Solu√ß√£o
>Para resolver este problema, siga estas etapas:
>1. **Verifique a ordem das tasks**: Confirme que as depend√™ncias est√£o corretamente configuradas:
   ```python
   # task_1 >> task_2 >> task_3
   ```

>2. **Confirme o xcom_push**: Certifique-se de que a task anterior est√° enviando os dados:
>3. **Verifique o xcom_pull**: Garanta que a chave est√° correta ao recuperar os dados:
>4. **Consulte os logs**: Verifique os logs da task anterior para confirmar se ela foi executada com sucesso
>5. **Teste com valores padr√£o**: Adicione tratamento para casos onde o XCom esteja vazio:

---

## Dicas Gerais de Troubleshooting

>- Sempre consulte os **logs das tasks** no Airflow UI clicando sobre a task espec√≠fica
>- Utilize o modo **Debug** para identificar problemas em desenvolvimento
>- Verifique as **conex√µes e vari√°veis** configuradas no Airflow Admin
>- Teste as queries SQL **diretamente no banco** antes de executar via Airflow
>- Mantenha os **logs detalhados** habilitados durante o desenvolvimento
>- Caso aparece outro erro ou as dicas n√£o funcionem, fique a vontade para pesquisar na internet


# üìß **Contato**

>**Henrico Fernandes**
> - [LinkedIn](https://www.linkedin.com/in/henricofernandes/) 
> - [Portf√≥lio](https://henri-fernandes.github.io/Portf-lio-Profissional-/)
>- Gmail: fernandes.henri2020@gmail.com

